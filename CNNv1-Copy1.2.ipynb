{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 1\n",
    "\n",
    "## The approach:\n",
    "\n",
    "### Without time feature\n",
    "### Pass 1x29 vectors into a convolutional layer, with kernel size 29, with some D number of filters\n",
    "### Add extra conv and dense layer to the model to see the effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First run: single conv layer single dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HarryG1/Documents/University/Year_3/DeepLearning-CreditCardFraud/devEnv/lib/python2.7/site-packages/ipykernel_launcher.py:21: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/Users/HarryG1/Documents/University/Year_3/DeepLearning-CreditCardFraud/devEnv/lib/python2.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import keras\n",
    "\n",
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# Normalise and reshape the Amount column, so it's values lie between -1 and 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data['norm_Amount'] = StandardScaler().fit_transform(data['Amount'].reshape(-1,1))\n",
    "\n",
    "# Drop the old Amount column and also the Time column as we don't want to include this at this stage\n",
    "data = data.drop(['Time', 'Amount'], axis=1)\n",
    "\n",
    "# Assign variables x and y corresponding to row data and it's class value\n",
    "X = data.ix[:, data.columns != 'Class']\n",
    "y = data.ix[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "def f1_score_custom(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    f1 score\n",
    "\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tp_3d = K.concatenate(\n",
    "        [\n",
    "            K.cast(y_true, 'bool'),\n",
    "            K.cast(K.round(y_pred), 'bool'),\n",
    "            K.cast(K.ones_like(y_pred), 'bool')\n",
    "        ], axis=1\n",
    "    )\n",
    "\n",
    "    fp_3d = K.concatenate(\n",
    "        [\n",
    "            K.cast(K.abs(y_true - K.ones_like(y_true)), 'bool'),\n",
    "            K.cast(K.round(y_pred), 'bool'),\n",
    "            K.cast(K.ones_like(y_pred), 'bool')\n",
    "        ], axis=1\n",
    "    )\n",
    "\n",
    "    fn_3d = K.concatenate(\n",
    "        [\n",
    "            K.cast(y_true, 'bool'),\n",
    "            K.cast(K.abs(K.round(y_pred) - K.ones_like(y_pred)), 'bool'),\n",
    "            K.cast(K.ones_like(y_pred), 'bool')\n",
    "        ], axis=1\n",
    "    )\n",
    "\n",
    "    tp = K.sum(K.cast(K.all(tp_3d, axis=1), 'int32'))\n",
    "    fp = K.sum(K.cast(K.all(fp_3d, axis=1), 'int32'))\n",
    "    fn = K.sum(K.cast(K.all(fn_3d, axis=1), 'int32'))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        score = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        predict = np.round(np.asarray(self.model.predict(self.validation_data[0])))\n",
    "        targ = self.validation_data[1]\n",
    "        \n",
    "        val_f1_score = f1_score(targ, predict, average=None)\n",
    "        val_precision_score = precision_score(targ, predict,average=None)\n",
    "        val_recall_score = recall_score(targ, predict, average=None)\n",
    "        \n",
    "        self.precision.append(val_precision_score)\n",
    "        self.recall.append(val_recall_score)\n",
    "        self.f1s.append(val_f1_score)\n",
    "        \n",
    "        print ' — val_f1: {} — val_precision: {} — val_recall {}'.format(val_f1_score, val_precision_score, val_recall_score)\n",
    "                \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model\n",
    "def create_model():\n",
    "    # create model\n",
    "    seed(2017)\n",
    "    conv = Sequential()\n",
    "    conv.add(Conv1D(256, 29, input_shape=(29, 1), activation='relu'))\n",
    "    conv.add(Flatten())\n",
    "    conv.add(Dense(300, activation = 'relu'))\n",
    "    conv.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    \n",
    "    # Compile model\n",
    "    conv.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "    return conv\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up dataframe table properties\n",
    "log_cols=[\"Name\", \"F1 Score\", \"Precision\", \"Recall\", \"Training Time\"]\n",
    "log = pd.DataFrame(columns=log_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_data(x_data, y_data):\n",
    "    from collections import Counter\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE()\n",
    "    X_res, y_res = sm.fit_sample(x_data, y_data)\n",
    "    print('Resampling the data with SMOTE. . .')\n",
    "    print('Resampled training dataset shape {}'.format(Counter(y_res)))\n",
    "\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_val(X, y, model, n):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.base import clone\n",
    "    import datetime\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    print 'Cross validating... \\n'\n",
    "    skfolds = StratifiedKFold(n_splits=n, random_state=42)\n",
    "    \n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1score = []\n",
    "    elapsed_times = []\n",
    "    cv = 0\n",
    "    \n",
    "    for train_index, test_index in skfolds.split(X, y):\n",
    "        cv=cv+1\n",
    "        print len(train_index)\n",
    "        clone_clf = clone(model)\n",
    "        X_train_folds = X.iloc[train_index]\n",
    "        y_train_folds = y.iloc[train_index]\n",
    "        X_test_fold = X.iloc[test_index]\n",
    "        y_test_fold = y.iloc[test_index]\n",
    "        \n",
    "        print len(y_train_folds[y_train_folds['Class']==1])\n",
    "        X_res, y_res = smote_data(X_train_folds, y_train_folds )\n",
    "        \n",
    "        \n",
    "        \n",
    "        print X_res.shape, type(X_res)\n",
    "        print y_res.shape\n",
    "\n",
    "        X_train = X_res.reshape(X_res.shape[0], 29, 1)\n",
    "        Y_train = y_res.reshape(y_res.shape[0], 1)\n",
    "        X_test = X_test_fold.values.reshape(X_test_fold.values.shape[0], 29, 1)\n",
    "        Y_test = y_test_fold.values.reshape(y_test_fold.values.shape[0], 1)\n",
    "\n",
    "        Y_test = keras.utils.to_categorical(Y_test)\n",
    "        Y_train = keras.utils.to_categorical(Y_train)\n",
    "        print Y_test.shape\n",
    "        print Y_train.shape\n",
    "\n",
    "        \n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        print('Fitting the model... CV[{}]'.format(cv))\n",
    "        clone_clf.fit(X_train, Y_train)\n",
    "        \n",
    "        end = datetime.datetime.now()\n",
    "        elapsed = end - start\n",
    "        elapsed_times.append(elapsed)\n",
    "        \n",
    "        y_pred = clone_clf.predict(X_test)\n",
    "        \n",
    "        # Set cut off point for class boundaries\n",
    "        cutt_off_tr = 0.5\n",
    "        y_pred[np.where(y_pred>=cutt_off_tr)] = 1\n",
    "        y_pred[np.where(y_pred<cutt_off_tr)]  = 0\n",
    "        \n",
    "        prfs = precision_recall_fscore_support(Y_test, y_pred, pos_label=1, average='binary')\n",
    "        \n",
    "        precision.append(prfs[0])\n",
    "        recall.append(prfs[1])\n",
    "        f1score.append(prfs[2])  \n",
    "    \n",
    "    average_timedelta = sum(elapsed_times, datetime.timedelta(0)) / len(elapsed_times)\n",
    "    entry = [name, np.mean(f1score), np.mean(precision), np.mean(recall), average_timedelta]\n",
    "    print('Mean scores: ', entry )\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_val_2(X, y, create_model, n):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.base import clone\n",
    "    import datetime\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    print 'Cross validating... \\n'\n",
    "    skfolds = StratifiedKFold(n_splits=n, random_state=42)\n",
    "    \n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1score = []\n",
    "    elapsed_times = []\n",
    "    cv = 0\n",
    "    \n",
    "    for train_index, test_index in skfolds.split(X, y):\n",
    "        cv=cv+1\n",
    "        print len(train_index)\n",
    "        clone_clf = create_model()\n",
    "        X_train_folds = X.iloc[train_index]\n",
    "        y_train_folds = y.iloc[train_index]\n",
    "        X_test_fold = X.iloc[test_index]\n",
    "        y_test_fold = y.iloc[test_index]\n",
    "        \n",
    "        print len(y_train_folds[y_train_folds['Class']==1])\n",
    "        X_res, y_res = smote_data(X_train_folds, y_train_folds )\n",
    "        \n",
    "        \n",
    "        \n",
    "        print X_res.shape, type(X_res)\n",
    "        print y_res.shape\n",
    "\n",
    "        X_train = X_res.reshape(X_res.shape[0], 29, 1)\n",
    "        Y_train = y_res.reshape(y_res.shape[0], 1)\n",
    "        X_test = X_test_fold.values.reshape(X_test_fold.values.shape[0], 29, 1)\n",
    "        Y_test = y_test_fold.values.reshape(y_test_fold.values.shape[0], 1)\n",
    "\n",
    "        Y_test = keras.utils.to_categorical(Y_test)\n",
    "        Y_train = keras.utils.to_categorical(Y_train)\n",
    "        print Y_test.shape\n",
    "        print Y_train.shape\n",
    "        \n",
    "        # metrics = Metrics()\n",
    "        \n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        print('Fitting the model... CV[{}]'.format(cv))\n",
    "        # clone_clf.fit(X_train, Y_train, callbacks=[metrics], validation_data=(X_test, Y_test))\n",
    "        \n",
    "        clone_clf.fit(X_train, Y_train, batch_size = 500, epochs = 25, verbose =1)\n",
    "        end = datetime.datetime.now()\n",
    "        elapsed = end - start\n",
    "        elapsed_times.append(elapsed)\n",
    "        \n",
    "        y_pred = clone_clf.predict(X_test)\n",
    "        \n",
    "        # Set cut off point for class boundaries\n",
    "        cutt_off_tr = 0.5\n",
    "        y_pred[np.where(y_pred>=cutt_off_tr)] = 1\n",
    "        y_pred[np.where(y_pred<cutt_off_tr)]  = 0\n",
    "        \n",
    "        prfs = precision_recall_fscore_support(Y_test, y_pred, labels=[0])\n",
    "        \n",
    "        precision.append(prfs[0][1])\n",
    "        recall.append(prfs[1][1])\n",
    "        f1score.append(prfs[2][1])  \n",
    "    \n",
    "    average_timedelta = sum(elapsed_times, datetime.timedelta(0)) / len(elapsed_times)\n",
    "    entry = ['CNN Model 1', np.mean(f1score), np.mean(precision), np.mean(recall), average_timedelta]\n",
    "    print('Mean scores: ', entry )\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validating... \n",
      "\n",
      "189871\n",
      "328\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 189543, 1: 189543})\n",
      "(379086, 29) <type 'numpy.ndarray'>\n",
      "(379086,)\n",
      "(94936, 2)\n",
      "(379086, 2)\n",
      "Fitting the model... CV[1]\n",
      "Epoch 1/25\n",
      "379086/379086 [==============================] - 5s 13us/step - loss: 0.0315\n",
      "Epoch 2/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0054\n",
      "Epoch 3/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0037\n",
      "Epoch 4/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0040\n",
      "Epoch 5/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0030\n",
      "Epoch 6/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0024\n",
      "Epoch 7/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0025\n",
      "Epoch 8/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 9/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0024\n",
      "Epoch 10/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0021\n",
      "Epoch 11/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0020\n",
      "Epoch 12/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0030\n",
      "Epoch 13/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0028\n",
      "Epoch 14/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0019\n",
      "Epoch 15/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0021\n",
      "Epoch 16/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 17/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 18/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 19/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 20/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0017\n",
      "Epoch 21/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0017\n",
      "Epoch 22/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 23/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0017\n",
      "Epoch 24/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0022\n",
      "Epoch 25/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0017\n",
      "189871\n",
      "328\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 189543, 1: 189543})\n",
      "(379086, 29) <type 'numpy.ndarray'>\n",
      "(379086,)\n",
      "(94936, 2)\n",
      "(379086, 2)\n",
      "Fitting the model... CV[2]\n",
      "Epoch 1/25\n",
      "379086/379086 [==============================] - 5s 13us/step - loss: 0.0298\n",
      "Epoch 2/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0069\n",
      "Epoch 3/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0045\n",
      "Epoch 4/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0042\n",
      "Epoch 5/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0066\n",
      "Epoch 6/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0040\n",
      "Epoch 7/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0035\n",
      "Epoch 8/25\n",
      "379086/379086 [==============================] - 5s 12us/step - loss: 0.0034\n",
      "Epoch 9/25\n",
      "379086/379086 [==============================] - 4s 12us/step - loss: 0.0034\n",
      "Epoch 10/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 11/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 12/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0040\n",
      "Epoch 13/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 14/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0030\n",
      "Epoch 15/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0030\n",
      "Epoch 16/25\n",
      "379086/379086 [==============================] - ETA: 0s - loss: 0.002 - 4s 11us/step - loss: 0.0029\n",
      "Epoch 17/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0029\n",
      "Epoch 18/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 19/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0030\n",
      "Epoch 20/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0029\n",
      "Epoch 21/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0029\n",
      "Epoch 22/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0028\n",
      "Epoch 23/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0028\n",
      "Epoch 24/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0028\n",
      "Epoch 25/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0028\n",
      "189872\n",
      "328\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 189544, 1: 189544})\n",
      "(379088, 29) <type 'numpy.ndarray'>\n",
      "(379088,)\n",
      "(94935, 2)\n",
      "(379088, 2)\n",
      "Fitting the model... CV[3]\n",
      "Epoch 1/25\n",
      "379088/379088 [==============================] - 5s 13us/step - loss: 0.0326\n",
      "Epoch 2/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0057\n",
      "Epoch 3/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0048\n",
      "Epoch 4/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0040\n",
      "Epoch 5/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0038\n",
      "Epoch 6/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0047\n",
      "Epoch 7/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0036\n",
      "Epoch 8/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0035\n",
      "Epoch 9/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0033\n",
      "Epoch 10/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0034\n",
      "Epoch 11/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 12/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0034\n",
      "Epoch 13/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0033\n",
      "Epoch 14/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 15/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 16/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 17/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 18/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 19/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 20/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0031\n",
      "Epoch 21/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0036\n",
      "Epoch 22/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 23/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 24/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 25/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "('Mean scores: ', ['CNN Model 1', 0.69985924150390533, 0.63225730609836572, 0.82113821138211396, datetime.timedelta(0, 108, 112916)])\n",
      "Cross validation training results: \n",
      "             F1 Score  Precision    Recall   Training Time\n",
      "Name                                                      \n",
      "CNN Model 1  0.001150   0.000576  0.333333 00:03:06.678778\n",
      "CNN Model 1  0.697258   0.626006  0.823171 00:00:43.629459\n",
      "CNN Model 1  0.699859   0.632257  0.821138 00:01:48.112916\n"
     ]
    }
   ],
   "source": [
    "results = custom_cross_val_2(X, y, create_model, 3)\n",
    "\n",
    "log_entry = pd.DataFrame([results], columns=log_cols)\n",
    "log = log.append(log_entry)\n",
    "\n",
    "# Replace table index by the Classifier column\n",
    "log.set_index('Name', inplace=True)\n",
    "print 'Cross validation training results: '\n",
    "print log \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validating... \n",
      "\n",
      "189871\n",
      "328\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 189543, 1: 189543})\n",
      "(379086, 29) <type 'numpy.ndarray'>\n",
      "(379086,)\n",
      "(94936, 2)\n",
      "(379086, 2)\n",
      "Fitting the model... CV[1]\n",
      "Epoch 1/25\n",
      "379086/379086 [==============================] - 5s 14us/step - loss: 0.0315\n",
      "Epoch 2/25\n",
      "379086/379086 [==============================] - 4s 12us/step - loss: 0.0053\n",
      "Epoch 3/25\n",
      "379086/379086 [==============================] - 5s 12us/step - loss: 0.0039\n",
      "Epoch 4/25\n",
      "379086/379086 [==============================] - 5s 12us/step - loss: 0.0041\n",
      "Epoch 5/25\n",
      "379086/379086 [==============================] - 5s 12us/step - loss: 0.0033\n",
      "Epoch 6/25\n",
      "379086/379086 [==============================] - 5s 12us/step - loss: 0.0026\n",
      "Epoch 7/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0025\n",
      "Epoch 8/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0022\n",
      "Epoch 9/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0021\n",
      "Epoch 10/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0021\n",
      "Epoch 11/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0020\n",
      "Epoch 12/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0020\n",
      "Epoch 13/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0019\n",
      "Epoch 14/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 15/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0019\n",
      "Epoch 16/25\n",
      "379086/379086 [==============================] - 5s 12us/step - loss: 0.0018\n",
      "Epoch 17/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 18/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 19/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 20/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0017\n",
      "Epoch 21/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0017\n",
      "Epoch 22/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0017\n",
      "Epoch 23/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0018\n",
      "Epoch 24/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0020\n",
      "Epoch 25/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0045\n",
      "189871\n",
      "328\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 189543, 1: 189543})\n",
      "(379086, 29) <type 'numpy.ndarray'>\n",
      "(379086,)\n",
      "(94936, 2)\n",
      "(379086, 2)\n",
      "Fitting the model... CV[2]\n",
      "Epoch 1/25\n",
      "379086/379086 [==============================] - 5s 13us/step - loss: 0.0298\n",
      "Epoch 2/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0070\n",
      "Epoch 3/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0046\n",
      "Epoch 4/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0041\n",
      "Epoch 5/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0067\n",
      "Epoch 6/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0038\n",
      "Epoch 7/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0035\n",
      "Epoch 8/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0034\n",
      "Epoch 9/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0035\n",
      "Epoch 10/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 11/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 12/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 13/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0030\n",
      "Epoch 14/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0030\n",
      "Epoch 15/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0030\n",
      "Epoch 16/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0030\n",
      "Epoch 17/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0029\n",
      "Epoch 18/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0029\n",
      "Epoch 19/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0029\n",
      "Epoch 20/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0029\n",
      "Epoch 21/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0029\n",
      "Epoch 22/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0028\n",
      "Epoch 23/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0028\n",
      "Epoch 24/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0028\n",
      "Epoch 25/25\n",
      "379086/379086 [==============================] - 4s 11us/step - loss: 0.0028\n",
      "189872\n",
      "328\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 189544, 1: 189544})\n",
      "(379088, 29) <type 'numpy.ndarray'>\n",
      "(379088,)\n",
      "(94935, 2)\n",
      "(379088, 2)\n",
      "Fitting the model... CV[3]\n",
      "Epoch 1/25\n",
      "379088/379088 [==============================] - 5s 13us/step - loss: 0.0318\n",
      "Epoch 2/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0060\n",
      "Epoch 3/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0051\n",
      "Epoch 4/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0042\n",
      "Epoch 5/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0038\n",
      "Epoch 6/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0044\n",
      "Epoch 7/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0036\n",
      "Epoch 8/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0036\n",
      "Epoch 9/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0034\n",
      "Epoch 10/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0034\n",
      "Epoch 11/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 12/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0034\n",
      "Epoch 13/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0033\n",
      "Epoch 14/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 15/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 16/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 17/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0031\n",
      "Epoch 18/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0033\n",
      "Epoch 19/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 20/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0031\n",
      "Epoch 21/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0037\n",
      "Epoch 22/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0032\n",
      "Epoch 23/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "Epoch 24/25\n",
      "379088/379088 [==============================] - 4s 12us/step - loss: 0.0031\n",
      "Epoch 25/25\n",
      "379088/379088 [==============================] - 4s 11us/step - loss: 0.0031\n",
      "('Mean scores: ', ['CNN Model 1', 0.68261313612557617, 0.62344999835683057, 0.7967479674796748, datetime.timedelta(0, 108, 29946)])\n",
      "['CNN Model 1', 0.68261313612557617, 0.62344999835683057, 0.7967479674796748, datetime.timedelta(0, 108, 29946)]\n"
     ]
    }
   ],
   "source": [
    "results2 = custom_cross_val_2(X, y, create_model, 3)\n",
    "print results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation training results: \n",
      "             F1 Score  Precision    Recall   Training Time\n",
      "Name                                                      \n",
      "NaN          0.001150   0.000576  0.333333 00:03:06.678778\n",
      "NaN          0.697258   0.626006  0.823171 00:00:43.629459\n",
      "NaN          0.699859   0.632257  0.821138 00:01:48.112916\n",
      "CNN Model 1  0.682613   0.623450  0.796748 00:01:48.029946\n",
      "CNN Model 1  0.682613   0.623450  0.796748 00:01:48.029946\n"
     ]
    }
   ],
   "source": [
    "log_entry = pd.DataFrame([results2], columns=log_cols)\n",
    "log = log.append(log_entry)\n",
    "\n",
    "# Replace table index by the Classifier column\n",
    "log.set_index('Name', inplace=True)\n",
    "print 'Cross validation training results: '\n",
    "print log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             F1 Score  Precision    Recall   Training Time\n",
      "Name                                                      \n",
      "NaN          0.001150   0.000576  0.333333 00:03:06.678778\n",
      "NaN          0.697258   0.626006  0.823171 00:00:43.629459\n",
      "NaN          0.699859   0.632257  0.821138 00:01:48.112916\n",
      "CNN Model 1  0.682613   0.623450  0.796748 00:01:48.029946\n",
      "CNN Model 1  0.682613   0.623450  0.796748 00:01:48.029946\n"
     ]
    }
   ],
   "source": [
    "print log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model_2():\n",
    "    # create model\n",
    "    seed(2017)\n",
    "    conv = Sequential()\n",
    "    conv.add(Conv1D(256, 29, input_shape=(29, 1), activation='relu'))\n",
    "    conv.add(Conv1D(256, 1, activation='relu'))\n",
    "    conv.add(Flatten())\n",
    "\n",
    "    conv.add(Dense(300, activation = 'relu'))\n",
    "    conv.add(Dense(100, activation = 'relu'))\n",
    "    conv.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    \n",
    "    # Compile model\n",
    "    conv.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "    return conv\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_val_2(X, y, create_model, n):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.base import clone\n",
    "    import datetime\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    print 'Cross validating... \\n'\n",
    "    skfolds = StratifiedKFold(n_splits=n, random_state=42)\n",
    "    \n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1score = []\n",
    "    elapsed_times = []\n",
    "    cv = 0\n",
    "    \n",
    "    for train_index, test_index in skfolds.split(X, y):\n",
    "        cv=cv+1\n",
    "        print len(train_index)\n",
    "        clone_clf = create_model()\n",
    "        X_train_folds = X.iloc[train_index]\n",
    "        y_train_folds = y.iloc[train_index]\n",
    "        X_test_fold = X.iloc[test_index]\n",
    "        y_test_fold = y.iloc[test_index]\n",
    "        \n",
    "        print len(y_train_folds[y_train_folds['Class']==1])\n",
    "        X_res, y_res = smote_data(X_train_folds, y_train_folds )\n",
    "        \n",
    "        \n",
    "        \n",
    "        print X_res.shape, type(X_res)\n",
    "        print y_res.shape\n",
    "\n",
    "        X_train = X_res.reshape(X_res.shape[0], 29, 1)\n",
    "        Y_train = y_res.reshape(y_res.shape[0], 1)\n",
    "        X_test = X_test_fold.values.reshape(X_test_fold.values.shape[0], 29, 1)\n",
    "        Y_test = y_test_fold.values.reshape(y_test_fold.values.shape[0], 1)\n",
    "\n",
    "        Y_test = keras.utils.to_categorical(Y_test)\n",
    "        Y_train = keras.utils.to_categorical(Y_train)\n",
    "        print Y_test.shape\n",
    "        print Y_train.shape\n",
    "        \n",
    "        # metrics = Metrics()\n",
    "        \n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        print('Fitting the model... CV[{}]'.format(cv))\n",
    "        # clone_clf.fit(X_train, Y_train, callbacks=[metrics], validation_data=(X_test, Y_test))\n",
    "        \n",
    "        clone_clf.fit(X_train, Y_train, batch_size = 500, epochs = 25, verbose =1)\n",
    "        end = datetime.datetime.now()\n",
    "        elapsed = end - start\n",
    "        elapsed_times.append(elapsed)\n",
    "        \n",
    "        y_pred = clone_clf.predict(X_test)\n",
    "        \n",
    "        # Set cut off point for class boundaries\n",
    "        cutt_off_tr = 0.5\n",
    "        y_pred[np.where(y_pred>=cutt_off_tr)] = 1\n",
    "        y_pred[np.where(y_pred<cutt_off_tr)]  = 0\n",
    "        \n",
    "        prfs = precision_recall_fscore_support(Y_test, y_pred, labels=[0])\n",
    "        \n",
    "        precision.append(prfs[0][1])\n",
    "        recall.append(prfs[1][1])\n",
    "        f1score.append(prfs[2][1])  \n",
    "    \n",
    "    average_timedelta = sum(elapsed_times, datetime.timedelta(0)) / len(elapsed_times)\n",
    "    entry = ['CNN Model 1.2', np.mean(f1score), np.mean(precision), np.mean(recall), average_timedelta]\n",
    "    print('Mean scores: ', entry )\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validating... \n",
      "\n",
      "189871\n",
      "328\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 189543, 1: 189543})\n",
      "(379086, 29) <type 'numpy.ndarray'>\n",
      "(379086,)\n",
      "(94936, 2)\n",
      "(379086, 2)\n",
      "Fitting the model... CV[1]\n",
      "Epoch 1/25\n",
      "379086/379086 [==============================] - 11s 29us/step - loss: 0.0295\n",
      "Epoch 2/25\n",
      "379086/379086 [==============================] - 10s 27us/step - loss: 0.0053\n",
      "Epoch 3/25\n",
      "379086/379086 [==============================] - 9s 25us/step - loss: 0.0033\n",
      "Epoch 4/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0031\n",
      "Epoch 5/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0057\n",
      "Epoch 6/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0028\n",
      "Epoch 7/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0028\n",
      "Epoch 8/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0023\n",
      "Epoch 9/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0020\n",
      "Epoch 10/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0023\n",
      "Epoch 11/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0019\n",
      "Epoch 12/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0019\n",
      "Epoch 13/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0019\n",
      "Epoch 14/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0020\n",
      "Epoch 15/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0017\n",
      "Epoch 16/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0017\n",
      "Epoch 17/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0017\n",
      "Epoch 18/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0017\n",
      "Epoch 19/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0017\n",
      "Epoch 20/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0020\n",
      "Epoch 21/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0017\n",
      "Epoch 22/25\n",
      "379086/379086 [==============================] - 9s 25us/step - loss: 0.0022\n",
      "Epoch 23/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0018\n",
      "Epoch 24/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0018\n",
      "Epoch 25/25\n",
      "379086/379086 [==============================] - 9s 24us/step - loss: 0.0017\n",
      "189871\n",
      "328\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 189543, 1: 189543})\n",
      "(379086, 29) <type 'numpy.ndarray'>\n",
      "(379086,)\n",
      "(94936, 2)\n",
      "(379086, 2)\n",
      "Fitting the model... CV[2]\n",
      "Epoch 1/25\n",
      "379086/379086 [==============================] - 11s 28us/step - loss: 0.0264\n",
      "Epoch 2/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0060\n",
      "Epoch 3/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0058\n",
      "Epoch 4/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0041\n",
      "Epoch 5/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0040\n",
      "Epoch 6/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0037\n",
      "Epoch 7/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0033\n",
      "Epoch 8/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0035\n",
      "Epoch 9/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0031\n",
      "Epoch 10/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0032\n",
      "Epoch 11/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0031\n",
      "Epoch 12/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0030\n",
      "Epoch 13/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0032\n",
      "Epoch 14/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0033\n",
      "Epoch 15/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0030\n",
      "Epoch 16/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0031\n",
      "Epoch 17/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0029\n",
      "Epoch 18/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0031\n",
      "Epoch 19/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0029\n",
      "Epoch 20/25\n",
      "379086/379086 [==============================] - 10s 26us/step - loss: 0.0028\n",
      "Epoch 21/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0029\n",
      "Epoch 22/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0029\n",
      "Epoch 23/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0028\n",
      "Epoch 24/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0028\n",
      "Epoch 25/25\n",
      "379086/379086 [==============================] - 10s 25us/step - loss: 0.0027\n",
      "189872\n",
      "328\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 189544, 1: 189544})\n",
      "(379088, 29) <type 'numpy.ndarray'>\n",
      "(379088,)\n",
      "(94935, 2)\n",
      "(379088, 2)\n",
      "Fitting the model... CV[3]\n",
      "Epoch 1/25\n",
      "379088/379088 [==============================] - 10s 26us/step - loss: 0.0287\n",
      "Epoch 2/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0053\n",
      "Epoch 3/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0046\n",
      "Epoch 4/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0042\n",
      "Epoch 5/25\n",
      "379088/379088 [==============================] - 10s 26us/step - loss: 0.0043\n",
      "Epoch 6/25\n",
      "379088/379088 [==============================] - 9s 25us/step - loss: 0.0035\n",
      "Epoch 7/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0035\n",
      "Epoch 8/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0036\n",
      "Epoch 9/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0033\n",
      "Epoch 10/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0032\n",
      "Epoch 11/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0033\n",
      "Epoch 12/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0031\n",
      "Epoch 13/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0033\n",
      "Epoch 14/25\n",
      "379088/379088 [==============================] - 9s 25us/step - loss: 0.0031\n",
      "Epoch 15/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0031\n",
      "Epoch 16/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0031\n",
      "Epoch 17/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0031\n",
      "Epoch 18/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0031\n",
      "Epoch 19/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0031\n",
      "Epoch 20/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0031\n",
      "Epoch 21/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0030\n",
      "Epoch 22/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0030\n",
      "Epoch 23/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0030\n",
      "Epoch 24/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0030\n",
      "Epoch 25/25\n",
      "379088/379088 [==============================] - 9s 24us/step - loss: 0.0030\n",
      "('Mean scores: ', ['CNN Model 1.2', 0.74746551464817712, 0.70514150227174499, 0.80691056910569114, datetime.timedelta(0, 236, 400232)])\n",
      "['CNN Model 1.2', 0.74746551464817712, 0.70514150227174499, 0.80691056910569114, datetime.timedelta(0, 236, 400232)]\n"
     ]
    }
   ],
   "source": [
    "results1_2 = custom_cross_val_2(X, y, create_model_2, 3)\n",
    "print results1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up dataframe table properties\n",
    "log_cols=[\"Name\", \"F1 Score\", \"Precision\", \"Recall\", \"Training Time\"]\n",
    "log = pd.DataFrame(columns=log_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation training results: \n",
      "               F1 Score  Precision    Recall   Training Time\n",
      "Name                                                        \n",
      "CNN Model 1.2  0.747466   0.705142  0.806911 00:03:56.400232\n"
     ]
    }
   ],
   "source": [
    "log_entry = pd.DataFrame([results1_2], columns=log_cols)\n",
    "log = log.append(log_entry)\n",
    "\n",
    "# Replace table index by the Classifier column\n",
    "log.set_index('Name', inplace=True)\n",
    "print 'Cross validation training results: '\n",
    "print log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HarryG1/Documents/University/Year_3/DeepLearning-CreditCardFraud/devEnv/lib/python2.7/site-packages/ipykernel_launcher.py:18: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "/Users/HarryG1/Documents/University/Year_3/DeepLearning-CreditCardFraud/devEnv/lib/python2.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 199015, 1: 199015})\n",
      "(398030, 29) <type 'numpy.ndarray'>\n",
      "(398030,)\n",
      "(85443, 2)\n",
      "(398030, 2)\n",
      "Epoch 1/10\n",
      "398030/398030 [==============================] - 5s 14us/step - loss: 0.0333 - acc: 0.9886\n",
      "Epoch 2/10\n",
      "398030/398030 [==============================] - 5s 12us/step - loss: 0.0066 - acc: 0.9989\n",
      "Epoch 3/10\n",
      "398030/398030 [==============================] - 5s 12us/step - loss: 0.0050 - acc: 0.9992\n",
      "Epoch 4/10\n",
      "398030/398030 [==============================] - 5s 12us/step - loss: 0.0056 - acc: 0.9990\n",
      "Epoch 5/10\n",
      "398030/398030 [==============================] - 5s 12us/step - loss: 0.0039 - acc: 0.9995\n",
      "Epoch 6/10\n",
      "398030/398030 [==============================] - 5s 13us/step - loss: 0.0036 - acc: 0.9995\n",
      "Epoch 7/10\n",
      "398030/398030 [==============================] - 5s 12us/step - loss: 0.0032 - acc: 0.9996\n",
      "Epoch 8/10\n",
      "398030/398030 [==============================] - 5s 12us/step - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 9/10\n",
      "398030/398030 [==============================] - 5s 12us/step - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 10/10\n",
      "398030/398030 [==============================] - 5s 12us/step - loss: 0.0029 - acc: 0.9997\n",
      "85443/85443 [==============================] - 1s 7us/step\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "# from keras_diagram import ascii\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import keras\n",
    "\n",
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# Normalise and reshape the Amount column, so it's values lie between -1 and 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data['norm_Amount'] = StandardScaler().fit_transform(data['Amount'].reshape(-1,1))\n",
    "\n",
    "# Drop the old Amount column and also the Time column as we don't want to include this at this stage\n",
    "data = data.drop(['Time', 'Amount'], axis=1)\n",
    "\n",
    "# Assign variables x and y corresponding to row data and it's class value\n",
    "X = data.ix[:, data.columns != 'Class']\n",
    "y = data.ix[:, data.columns == 'Class']\n",
    "\n",
    "def generate_train_test_sample(x_data, y_data): \n",
    "    ''' 1) Generate new, random train-test split\n",
    "        2) Random smote oversample the train data, keeping test data unseen\n",
    "        3) Use this new train-test split to fit and test model\n",
    "    '''\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_data,y_data,test_size = 0.3)\n",
    "\n",
    "    from collections import Counter\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE()\n",
    "    X_res, y_res = sm.fit_sample(X_train, y_train)\n",
    "    print('Resampling the data with SMOTE. . .')\n",
    "    print('Resampled training dataset shape {}'.format(Counter(y_res)))\n",
    "\n",
    "    return X_res, y_res, X_test, y_test\n",
    "\n",
    "########################################################################\n",
    "\n",
    "X_res, y_res, X_test, y_test = generate_train_test_sample(X, y)\n",
    "\n",
    "print X_res.shape, type(X_res)\n",
    "print y_res.shape\n",
    "\n",
    "X_train = X_res.reshape(X_res.shape[0], 29, 1)\n",
    "Y_train = y_res.reshape(y_res.shape[0], 1)\n",
    "X_test = X_test.values.reshape(X_test.values.shape[0], 29, 1)\n",
    "Y_test = y_test.values.reshape(y_test.values.shape[0], 1)\n",
    "\n",
    "Y_test = keras.utils.to_categorical(Y_test)\n",
    "Y_train = keras.utils.to_categorical(Y_train)\n",
    "print Y_test.shape\n",
    "print Y_train.shape\n",
    "\n",
    "\n",
    "seed(2017)\n",
    "conv = Sequential()\n",
    "conv.add(Conv1D(256, 29, input_shape=(29, 1), activation='relu'))\n",
    "\n",
    "conv.add(Flatten())\n",
    "\n",
    "conv.add(Dense(300, activation = 'relu'))\n",
    "conv.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "conv.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "conv.fit(X_train, Y_train, batch_size = 500, epochs = 10, verbose = 1)\n",
    "score = conv.evaluate(X_test, Y_test, batch_size=500)\n",
    "\n",
    "y_pred = conv.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85443, 2)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "[Precision, Recall, F1, Support]\n",
      "====================================================================================================\n",
      "0:      0.999788863735         0.999237983587      0.999513347757      85300   \n",
      "1:      0.657894736842         0.874125874126      0.750750750751      143   \n",
      "====================================================================================================\n",
      "F1 Score, Fraud Class = 0.750750750751\n"
     ]
    }
   ],
   "source": [
    "cutt_off_tr = 0.5\n",
    "y_pred[np.where(y_pred>=cutt_off_tr)] = 1\n",
    "y_pred[np.where(y_pred<cutt_off_tr)]  = 0\n",
    "\n",
    "prfs0 = precision_recall_fscore_support(Y_test, y_pred, labels=[0])\n",
    "\n",
    "print 'Classification Report: \\n'\n",
    "print '[Precision, Recall, F1, Support]'\n",
    "print '='*100\n",
    "print '0:      {}         {}      {}      {}   '.format(prfs0[0][0], prfs0[1][0], prfs0[2][0], prfs0[3][0])\n",
    "print '1:      {}         {}      {}      {}   '.format(prfs0[0][1], prfs0[1][1], prfs0[2][1], prfs0[3][1])\n",
    "print '='*100\n",
    "\n",
    "print 'F1 Score, Fraud Class = {}'.format(prfs0[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 1, 256)            7680      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1, 256)            65792     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               77100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 180,874\n",
      "Trainable params: 180,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of first run\n",
    "\n",
    "We can see that this very simple model, with just a single convolution layer piped into a simple dense network, already gives comparable F1 to our top two baseline classifiers:\n",
    "\n",
    "CNNv1.1:                 0.776316\n",
    "\n",
    "RandomForestClassifier:  0.846437   \n",
    "MLPClassifier:           0.750672 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second run, with added conv and dense layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 199022, 1: 199022})\n",
      "(398044, 29) <type 'numpy.ndarray'>\n",
      "(398044,)\n",
      "(85443, 2)\n",
      "(398044, 2)\n",
      "Epoch 1/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0283 - acc: 0.9903\n",
      "Epoch 2/50\n",
      "398044/398044 [==============================] - 11s 27us/step - loss: 0.0054 - acc: 0.9990\n",
      "Epoch 3/50\n",
      "398044/398044 [==============================] - 11s 26us/step - loss: 0.0048 - acc: 0.9992\n",
      "Epoch 4/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0038 - acc: 0.9994\n",
      "Epoch 5/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0038 - acc: 0.9995\n",
      "Epoch 6/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 7/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 8/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 9/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 10/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0031 - acc: 0.9997\n",
      "Epoch 11/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 12/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 13/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 14/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 15/50\n",
      "398044/398044 [==============================] - 11s 27us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 17/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 18/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 19/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 20/50\n",
      "398044/398044 [==============================] - 9s 24us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 21/50\n",
      "398044/398044 [==============================] - 9s 24us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 22/50\n",
      "398044/398044 [==============================] - 9s 24us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 23/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 24/50\n",
      "398044/398044 [==============================] - 11s 27us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 25/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 26/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0032 - acc: 0.9997\n",
      "Epoch 27/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 28/50\n",
      "398044/398044 [==============================] - 11s 27us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 29/50\n",
      "398044/398044 [==============================] - 11s 27us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 30/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 31/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 32/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 33/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 34/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 35/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 36/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 37/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 38/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 39/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 40/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 41/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 42/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 43/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 44/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 45/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 46/50\n",
      "398044/398044 [==============================] - 10s 24us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 47/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 48/50\n",
      "398044/398044 [==============================] - 11s 26us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 49/50\n",
      "398044/398044 [==============================] - 10s 26us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 50/50\n",
      "398044/398044 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "85443/85443 [==============================] - 1s 8us/step\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# CNNv1.2\n",
    "\n",
    "X_res, y_res, X_test, y_test = generate_train_test_sample(X, y)\n",
    "\n",
    "print X_res.shape, type(X_res)\n",
    "print y_res.shape\n",
    "\n",
    "X_train = X_res.reshape(X_res.shape[0], 29, 1)\n",
    "Y_train = y_res.reshape(y_res.shape[0], 1)\n",
    "X_test = X_test.values.reshape(X_test.values.shape[0], 29, 1)\n",
    "Y_test = y_test.values.reshape(y_test.values.shape[0], 1)\n",
    "\n",
    "Y_test = keras.utils.to_categorical(Y_test)\n",
    "Y_train = keras.utils.to_categorical(Y_train)\n",
    "print Y_test.shape\n",
    "print Y_train.shape\n",
    "\n",
    "\n",
    "seed(2017)\n",
    "conv = Sequential()\n",
    "conv.add(Conv1D(256, 29, input_shape=(29, 1), activation='relu'))\n",
    "conv.add(Conv1D(256, 1, activation='relu'))\n",
    "conv.add(Flatten())\n",
    "\n",
    "conv.add(Dense(300, activation = 'relu'))\n",
    "conv.add(Dense(100, activation = 'relu'))\n",
    "conv.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "conv.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "conv.fit(X_train, Y_train, batch_size = 500, epochs = 50, verbose = 1)\n",
    "score = conv.evaluate(X_test, Y_test, batch_size=500)\n",
    "\n",
    "y_pred = conv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "\n",
      "[Precision, Recall, F1, Support]\n",
      "====================================================================================================\n",
      "0:      0.999718584009         0.999601374087      0.999659975612      85293   \n",
      "1:      0.7875         0.84      0.812903225806      150   \n",
      "====================================================================================================\n",
      "F1 Score, Fraud Class = 0.812903225806\n"
     ]
    }
   ],
   "source": [
    "y_pred[np.where(y_pred>=cutt_off_tr)] = 1\n",
    "y_pred[np.where(y_pred<cutt_off_tr)]  = 0\n",
    "\n",
    "prfs0 = precision_recall_fscore_support(Y_test, y_pred, labels=[0])\n",
    "\n",
    "print 'Classification Report: \\n'\n",
    "print '[Precision, Recall, F1, Support]'\n",
    "print '='*100\n",
    "print '0:      {}         {}      {}      {}   '.format(prfs0[0][0], prfs0[1][0], prfs0[2][0], prfs0[3][0])\n",
    "print '1:      {}         {}      {}      {}   '.format(prfs0[0][1], prfs0[1][1], prfs0[2][1], prfs0[3][1])\n",
    "print '='*100\n",
    "\n",
    "print 'F1 Score, Fraud Class = {}'.format(prfs0[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of second run\n",
    "\n",
    "This seems very promising, already matching our best baseline classifier, with our first CNN approach.\n",
    "\n",
    "#### However, we should at least average runs to get confidence in results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNv1 averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_run():\n",
    "    reports = []\n",
    "    for i in range(3):\n",
    "        X_res, y_res, X_test, y_test = generate_train_test_sample(X, y)\n",
    "\n",
    "        print X_res.shape, type(X_res)\n",
    "        print y_res.shape\n",
    "\n",
    "        X_train = X_res.reshape(X_res.shape[0], 29, 1)\n",
    "        Y_train = y_res.reshape(y_res.shape[0], 1)\n",
    "        X_test = X_test.values.reshape(X_test.values.shape[0], 29, 1)\n",
    "        Y_test = y_test.values.reshape(y_test.values.shape[0], 1)\n",
    "\n",
    "        Y_test = keras.utils.to_categorical(Y_test)\n",
    "        Y_train = keras.utils.to_categorical(Y_train)\n",
    "        print Y_test.shape\n",
    "        print Y_train.shape\n",
    "\n",
    "\n",
    "        seed(2017)\n",
    "        conv = Sequential()\n",
    "        conv.add(Conv1D(256, 29, input_shape=(29, 1), activation='relu'))\n",
    "        conv.add(Flatten())\n",
    "        conv.add(Dense(300, activation = 'relu'))\n",
    "        conv.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "        sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "        conv.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "        conv.fit(X_train, Y_train, batch_size = 500, epochs = 50, verbose = 1, callbacks=[keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')])\n",
    "        score = conv.evaluate(X_test, Y_test, batch_size=500)\n",
    "\n",
    "        y_pred = conv.predict(X_test)\n",
    "\n",
    "        cutt_off_tr = 0.5\n",
    "        y_pred[np.where(y_pred>=cutt_off_tr)] = 1\n",
    "        y_pred[np.where(y_pred<cutt_off_tr)]  = 0\n",
    "\n",
    "        prfs0 = precision_recall_fscore_support(Y_test, y_pred, labels=[0])\n",
    "        reports.append(prfs0)\n",
    "    \n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 199015, 1: 199015})\n",
      "(398030, 29) <type 'numpy.ndarray'>\n",
      "(398030,)\n",
      "(85443, 2)\n",
      "(398030, 2)\n",
      "Epoch 1/50\n",
      "398030/398030 [==============================] - 6s 14us/step - loss: 0.0313 - acc: 0.9895\n",
      "Epoch 2/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0060 - acc: 0.9989\n",
      "Epoch 3/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0044 - acc: 0.9993\n",
      "Epoch 4/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0038 - acc: 0.9995\n",
      "Epoch 5/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 6/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0064 - acc: 0.9990\n",
      "Epoch 7/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0041 - acc: 0.9994\n",
      "Epoch 8/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 9/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 10/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 11/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 12/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 13/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 14/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 15/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "398030/398030 [==============================] - 5s 11us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 00016: early stopping\n",
      "85443/85443 [==============================] - 1s 7us/step\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 199032, 1: 199032})\n",
      "(398064, 29) <type 'numpy.ndarray'>\n",
      "(398064,)\n",
      "(85443, 2)\n",
      "(398064, 2)\n",
      "Epoch 1/50\n",
      "398064/398064 [==============================] - 5s 13us/step - loss: 0.0311 - acc: 0.9899\n",
      "Epoch 2/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0069 - acc: 0.9987\n",
      "Epoch 3/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0047 - acc: 0.9993\n",
      "Epoch 4/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0043 - acc: 0.9993\n",
      "Epoch 5/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0078 - acc: 0.9989\n",
      "Epoch 6/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0038 - acc: 0.9995\n",
      "Epoch 7/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0032 - acc: 0.9996\n",
      "Epoch 8/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0035 - acc: 0.9996\n",
      "Epoch 9/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0032 - acc: 0.9996\n",
      "Epoch 10/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 11/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0028 - acc: 0.9998\n",
      "Epoch 12/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 13/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 14/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 15/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "398064/398064 [==============================] - 5s 12us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 00016: early stopping\n",
      "85443/85443 [==============================] - 1s 8us/step\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 199021, 1: 199021})\n",
      "(398042, 29) <type 'numpy.ndarray'>\n",
      "(398042,)\n",
      "(85443, 2)\n",
      "(398042, 2)\n",
      "Epoch 1/50\n",
      "398042/398042 [==============================] - 5s 13us/step - loss: 0.0328 - acc: 0.9886\n",
      "Epoch 2/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0061 - acc: 0.9989\n",
      "Epoch 3/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0049 - acc: 0.9992\n",
      "Epoch 4/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0040 - acc: 0.9995\n",
      "Epoch 5/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0040 - acc: 0.9994\n",
      "Epoch 6/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0037 - acc: 0.9995\n",
      "Epoch 7/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0032 - acc: 0.9996\n",
      "Epoch 8/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0032 - acc: 0.9996\n",
      "Epoch 9/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 10/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 11/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 12/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0059 - acc: 0.9992\n",
      "Epoch 13/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 14/50\n",
      "398042/398042 [==============================] - 5s 11us/step - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 00014: early stopping\n",
      "85443/85443 [==============================] - 1s 8us/step\n"
     ]
    }
   ],
   "source": [
    "reports = average_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 AVG = 0.771745237875\n"
     ]
    }
   ],
   "source": [
    "results = [reports[0][2][1],reports[1][2][1],reports[1][2][1]]\n",
    "avg_f1 = np.mean(results)\n",
    "print 'F1 AVG = {}'.format(avg_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNv1.2 averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_run():\n",
    "    reports = []\n",
    "    for i in range(3):\n",
    "        X_res, y_res, X_test, y_test = generate_train_test_sample(X, y)\n",
    "\n",
    "        print X_res.shape, type(X_res)\n",
    "        print y_res.shape\n",
    "\n",
    "        X_train = X_res.reshape(X_res.shape[0], 29, 1)\n",
    "        Y_train = y_res.reshape(y_res.shape[0], 1)\n",
    "        X_test = X_test.values.reshape(X_test.values.shape[0], 29, 1)\n",
    "        Y_test = y_test.values.reshape(y_test.values.shape[0], 1)\n",
    "\n",
    "        Y_test = keras.utils.to_categorical(Y_test)\n",
    "        Y_train = keras.utils.to_categorical(Y_train)\n",
    "        print Y_test.shape\n",
    "        print Y_train.shape\n",
    "\n",
    "\n",
    "        seed(2017)\n",
    "        conv = Sequential()\n",
    "        conv.add(Conv1D(256, 29, input_shape=(29, 1), activation='relu'))\n",
    "        conv.add(Conv1D(256, 1, activation='relu'))\n",
    "        conv.add(Flatten())\n",
    "\n",
    "        conv.add(Dense(300, activation = 'relu'))\n",
    "        conv.add(Dense(100, activation = 'relu'))\n",
    "        conv.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "        sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "        conv.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "        conv.fit(X_train, Y_train, batch_size = 500, epochs = 50, verbose = 1, callbacks=[keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=5, verbose=0, mode='auto')])\n",
    "        score = conv.evaluate(X_test, Y_test, batch_size=500)\n",
    "\n",
    "        y_pred = conv.predict(X_test)\n",
    "\n",
    "        cutt_off_tr = 0.5\n",
    "        y_pred[np.where(y_pred>=cutt_off_tr)] = 1\n",
    "        y_pred[np.where(y_pred<cutt_off_tr)]  = 0\n",
    "\n",
    "        prfs0 = precision_recall_fscore_support(Y_test, y_pred, labels=[0])\n",
    "        reports.append(prfs0)\n",
    "    \n",
    "    return reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 199028, 1: 199028})\n",
      "(398056, 29) <type 'numpy.ndarray'>\n",
      "(398056,)\n",
      "(85443, 2)\n",
      "(398056, 2)\n",
      "Epoch 1/50\n",
      "398056/398056 [==============================] - 11s 27us/step - loss: 0.0274 - acc: 0.9904\n",
      "Epoch 2/50\n",
      "398056/398056 [==============================] - 13s 32us/step - loss: 0.0114 - acc: 0.9980\n",
      "Epoch 3/50\n",
      "398056/398056 [==============================] - 12s 31us/step - loss: 0.0048 - acc: 0.9992 0s - loss: 0.0048 - a\n",
      "Epoch 4/50\n",
      "398056/398056 [==============================] - 12s 30us/step - loss: 0.0041 - acc: 0.9994\n",
      "Epoch 5/50\n",
      "398056/398056 [==============================] - 13s 32us/step - loss: 0.0037 - acc: 0.9995\n",
      "Epoch 6/50\n",
      "398056/398056 [==============================] - 12s 31us/step - loss: 0.0036 - acc: 0.9995\n",
      "Epoch 7/50\n",
      "398056/398056 [==============================] - 13s 31us/step - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 8/50\n",
      "398056/398056 [==============================] - 12s 30us/step - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 9/50\n",
      "398056/398056 [==============================] - 11s 27us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 10/50\n",
      "398056/398056 [==============================] - 11s 27us/step - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 11/50\n",
      "398056/398056 [==============================] - 10s 26us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 12/50\n",
      "398056/398056 [==============================] - 10s 26us/step - loss: 0.0028 - acc: 0.9998\n",
      "Epoch 13/50\n",
      "398056/398056 [==============================] - 10s 25us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 14/50\n",
      "398056/398056 [==============================] - 10s 25us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 15/50\n",
      "398056/398056 [==============================] - 10s 24us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "398056/398056 [==============================] - 10s 24us/step - loss: 0.0027 - acc: 0.9998\n",
      "85443/85443 [==============================] - 1s 10us/step\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 199033, 1: 199033})\n",
      "(398066, 29) <type 'numpy.ndarray'>\n",
      "(398066,)\n",
      "(85443, 2)\n",
      "(398066, 2)\n",
      "Epoch 1/50\n",
      "398066/398066 [==============================] - 10s 26us/step - loss: 0.0274 - acc: 0.9901\n",
      "Epoch 2/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0091 - acc: 0.9984\n",
      "Epoch 3/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0047 - acc: 0.9992\n",
      "Epoch 4/50\n",
      "398066/398066 [==============================] - 10s 25us/step - loss: 0.0040 - acc: 0.9994\n",
      "Epoch 5/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0032 - acc: 0.9997\n",
      "Epoch 6/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 7/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 8/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 9/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 10/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 11/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 12/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 13/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 14/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 15/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "398066/398066 [==============================] - 10s 25us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 17/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 18/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 19/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 20/50\n",
      "398066/398066 [==============================] - 10s 24us/step - loss: 0.0029 - acc: 0.9997\n",
      "85443/85443 [==============================] - 1s 10us/step\n",
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 199026, 1: 199026})\n",
      "(398052, 29) <type 'numpy.ndarray'>\n",
      "(398052,)\n",
      "(85443, 2)\n",
      "(398052, 2)\n",
      "Epoch 1/50\n",
      "398052/398052 [==============================] - 10s 26us/step - loss: 0.0266 - acc: 0.9904\n",
      "Epoch 2/50\n",
      "398052/398052 [==============================] - 10s 24us/step - loss: 0.0125 - acc: 0.9979\n",
      "Epoch 3/50\n",
      "398052/398052 [==============================] - 10s 24us/step - loss: 0.0054 - acc: 0.9990\n",
      "Epoch 4/50\n",
      "398052/398052 [==============================] - 10s 24us/step - loss: 0.0037 - acc: 0.9995\n",
      "Epoch 5/50\n",
      "398052/398052 [==============================] - 10s 24us/step - loss: 0.0035 - acc: 0.9995\n",
      "Epoch 6/50\n",
      "398052/398052 [==============================] - 10s 24us/step - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 7/50\n",
      "398052/398052 [==============================] - 10s 24us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 8/50\n",
      "398052/398052 [==============================] - 10s 25us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 9/50\n",
      "398052/398052 [==============================] - 10s 24us/step - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 10/50\n",
      "398052/398052 [==============================] - 10s 25us/step - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 11/50\n",
      "398052/398052 [==============================] - 10s 24us/step - loss: 0.0030 - acc: 0.9996\n",
      "Epoch 12/50\n",
      "398052/398052 [==============================] - 10s 25us/step - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 13/50\n",
      "398052/398052 [==============================] - 10s 25us/step - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 14/50\n",
      "398052/398052 [==============================] - 10s 25us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 15/50\n",
      "398052/398052 [==============================] - 10s 25us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "398052/398052 [==============================] - 10s 25us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 17/50\n",
      "398052/398052 [==============================] - 10s 25us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 18/50\n",
      "398052/398052 [==============================] - 10s 25us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 19/50\n",
      "398052/398052 [==============================] - 10s 24us/step - loss: 0.0023 - acc: 0.9998\n",
      "85443/85443 [==============================] - 1s 10us/step\n"
     ]
    }
   ],
   "source": [
    "reports = average_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 AVG = 0.810230099502\n"
     ]
    }
   ],
   "source": [
    "results = [reports[0][2][1],reports[1][2][1],reports[1][2][1]]\n",
    "avg_f1 = np.mean(results)\n",
    "print 'F1 AVG = {}'.format(avg_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNv1 : F1 AVG = 0.771745237875\n",
    "# CNNv1.2: F1 AVG = 0.810230099502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling the data with SMOTE. . .\n",
      "Resampled training dataset shape Counter({0: 199012, 1: 199012})\n",
      "(398024, 29) <type 'numpy.ndarray'>\n",
      "(398024,)\n"
     ]
    }
   ],
   "source": [
    "X_res, y_res, X_test, y_test = generate_train_test_sample(X, y)\n",
    "\n",
    "print X_res.shape, type(X_res)\n",
    "print y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11542696"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_res.reshape(X_res.shape[0], 29, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.15010044,  0.15115908, -2.19244029, ..., -0.07457871,\n",
       "        -0.07238514, -0.34995096],\n",
       "       [-0.68092548, -0.26758453, -1.95069919, ..., -0.53863108,\n",
       "        -0.09602917,  0.72397497],\n",
       "       [ 2.02016107, -0.19543356, -1.97548432, ..., -0.11447656,\n",
       "        -0.07892874, -0.19350585],\n",
       "       ..., \n",
       "       [-3.51735006,  1.72241489, -2.70740266, ...,  0.91308367,\n",
       "        -0.34269234,  0.95741203],\n",
       "       [-3.69408913,  0.19000357, -3.66340915, ...,  1.0578821 ,\n",
       "         0.04396139,  0.96994245],\n",
       "       [-2.01612965,  1.5548113 , -2.76132834, ..., -0.29483718,\n",
       "         0.66595781, -0.33692074]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_test = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
